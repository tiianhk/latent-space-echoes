<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Latent Space Echoes</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- title -->
          <h1 class="title is-1 publication-title">Latent Space Echoes</h1>
          <!-- author -->
          <div class="is-size-5 publication-authors">
              <span class="author-block">Haokun Tian</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Introduction -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <b>Latent Space Echoes</b> is a sound project / digital musical instrument I designed in the <b>Interactive Digital Multimedia Techniques</b> module (ECS742P) at Queen Mary University of London. The code for this project is available <a href="https://github.com/tiianhk/latent-space-echoes", target="_blank">here</a>. The video presentation is available <a href="https://youtu.be/QxdhR7yykOo", target="_blank">here</a>.
          </p><p>
            The term "latent space" usually refers to a high-dimensional space where generative AI models can learn to represent complex input data. In the context of representing audio and music data, the variational autoencoder <a href="https://github.com/acids-ircam/rave", target="_blank">RAVE</a> is the state-of-the-art and achieves realtime high-fidelity audio synthesis. <a href="https://acids-ircam.github.io/rave_models_download", target="_blank">Pretrained RAVE models</a> are available to run in Max with the <a href="https://github.com/acids-ircam/nn_tilde", target="_blank">nn~</a> external, allowing users to explore and manipulate their perceptually meaningful latent spaces creatively.
          </p><p>
            This project uses Arduino with the Ultrasonic Distance Sensor HC-SR04 to capture three dimensional distances between a custom-built fixed object and a moveable box. These distances are then mapped to a point in the latent space through interactive machine learning models using <a href="http://www.wekinator.org", target="_blank">Wekinator</a>. As the box is in motion, the RAVE decoder constantly generate sounds from the dynamic point in the latent space, creating what are referred to as "echoes." In addition, the angle of box rotation, captured by <a href="https://1-10.github.io/zigsim/", target="_blank">ZIG SIM</a>, is used to mix the "echoes" produced by the different RAVE decoders.
          </p><p>
            Below is a diagram of the main Max patch and its connections to other modules.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Introduction -->


<!-- Image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Your image here -->
      <img src="static/images/intro.jpg" alt="MY ALT TEXT"/>
    </div>
  </div>
</section>
<!-- End Image -->


<!-- Text -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Arduino & Distance Sensors</h2>
        <div class="content has-text-justified">
          <p>
            The components were integrated into a cube design. Positioned on the surfaces along the x, y, and z axes were three Ultrasonic Distance Sensors (HC-SR04). Updates to the distances occurred every 100 milliseconds, incorporating a moving average with a buffer size of 5 for a smoother output. Below are videos demonstrating the process of creating the cube using laser cutting and showing the final appearance of the device.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Text -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/laser.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/showcase.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Text -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RAVE models & Wekinator</h2>
        <div class="content has-text-justified">
          <p>
            Four RAVE models pretrained on the Percussion, SOL_full, Musicnet, and VCTK datasets were used. Wekinator was used to continuously map the 3D distances to the dimension size of each RAVE decoder. OSC was used for data collection and transmission between Max and Wekinator.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Text -->


<!-- Conclusion & Future Directions -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion & Future Directions</h2>
        <div class="content has-text-justified">
          <p>
            The idea worked. But before implementing this idea, I worked on a similar idea that required the use of the ESP32-CAM board, for which I didn't manage to upload the code to the board.
          </p><p>
            It has been a great experience. Through this module and this project, I learned Max, Arduino and sensors, interactive machine learning via Wekinator, laser cutting, and extensive knowledge on timbre and digital instrument making.
          </p><p>
            A future direction for this project is to utilize the tilt angles of the box (captured but not used) to shape the timbre of the generated sound. The angle along the x-axis can be used to control spectral effects and the angle along the y-axis can be used to control temporal effects.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Conclusion & Future Directions -->


<!-- Acknowledgement -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgement</h2>
        <div class="content has-text-justified">
          <p>
            Some course materials developed by Bleiz MacSen Del Sette and Ashley Noel-Hirst were modified and adapted for the purpose of this project. Thank you to Dr Charalampos Saitis for advice. Thank you to Bleiz MacSen Del Sette for long and fruitful discussions. Thank you to Pedro Sarmento for helping with the video.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Acknowledgement -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
             © 2023 Haokun Tian. Theme by <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
